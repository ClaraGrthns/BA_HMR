{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import zarr\n",
    "\n",
    "\n",
    "from modules.smpl_model.config_smpl import *\n",
    "from modules.utils.image_utils import to_tensor, transform\n",
    "from modules.utils.data_utils_h36m import get_data_list_h36m\n",
    "from modules.utils.geometry import get_smpl_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWiseH36M(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                data_path:str='../H36M',\n",
    "                split:str = 'train',\n",
    "                load_from_zarr:str=None,\n",
    "                load_datalist:str=None,\n",
    "                fitting_thr:int=None,\n",
    "                img_size=224):\n",
    "        super(ImageWiseH36M, self).__init__()\n",
    "        self.split = split\n",
    "        self.img_dir = osp.join(data_path, 'Human36M', 'images')\n",
    "        self.annot_dir = osp.join(data_path, 'Human36M', 'annotations')\n",
    "        self.load_from_zarr = load_from_zarr\n",
    "        self.fitting_thr = fitting_thr  # milimeter --> Threshhold joints from smpl mesh to h36m gt\n",
    "        self.subject_list = [1, 5, 6, 7, 8, 9, 11]\n",
    "        self.img_size = img_size\n",
    "        self.datalist = get_data_list_h36m(annot_dir=self.annot_dir,\n",
    "                                            subject_list=self.subject_list,\n",
    "                                            fitting_thr=self.fitting_thr,\n",
    "                                            load_from_pkl=load_datalist,\n",
    "                                            store_as_pkl=False,\n",
    "                                            out_dir=None,\n",
    "                                            )\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu' \n",
    "        if self.load_from_zarr is not None:\n",
    "            self.imgs = torch.from_numpy(zarr.load(self.load_from_zarr)) ### Load array into memory\n",
    "        elif self.store_images:\n",
    "            self.img_cache_indicator = torch.zeros(self.__len__(), dtype=torch.bool).to(self.device)\n",
    "            self.img_cache = torch.empty(self.__len__(), 3, img_size, img_size, dtype=torch.float32)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = self.datalist[index]\n",
    "        img_path = osp.joint(self.img_dir, data['img_name'])\n",
    "        if self.load_from_zarr is not None:\n",
    "            img_tensor = self.imgs[index]\n",
    "        elif self.store_images and self.img_cache_indicator[index]:\n",
    "            img_tensor = self.img_cache[index]\n",
    "        else:\n",
    "            ## Open Image\n",
    "            img_path = osp.join(self.img_dir, data['img_name'])\n",
    "            img = np.array(Image.open(img_path))\n",
    "            ## Open Mask\n",
    "            sub_dir, img_name = osp.split(data['img_name'])\n",
    "            mask_name = img_name.split('.')[-2]+'_mask.jpg'\n",
    "            mask_path = osp.join(self.img_dir, sub_dir, mask_name)\n",
    "            mask = np.array(Image.open(mask_path))\n",
    "\n",
    "            if data['bbox'] is not None:\n",
    "                x_min, y_min, x_max, y_max = data['bbox']\n",
    "                img = img[y_min:y_max, x_min:x_max]\n",
    "                mask = mask[y_min:y_max, x_min:x_max]\n",
    "                img == img[mask != 0, :]\n",
    "            img_tensor = to_tensor(img).to(self.device)\n",
    "            img_tensor = transform(img_tensor, img_size=self.img_size)\n",
    "            if self.store_images:\n",
    "                self.img_cache[index] = img_tensor\n",
    "                self.img_cache_indicator[index] = True\n",
    "        data['img_path'] = img_path\n",
    "        data['img'] = img_tensor\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a0426b8ba3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatalist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_list_h36m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../H36M/annotations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting_thr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../H36M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_as_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-286a7c4f8b1f>\u001b[0m in \u001b[0;36mget_data_list_h36m\u001b[0;34m(annot_dir, subject_list, fitting_thr, load_from_pkl, store_as_pkl, out_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mcam_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcameras\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mcam_pose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam_intr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cam_pose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cam_intr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 datalist.append({\n\u001b[1;32m     81\u001b[0m                     \u001b[0;34m'img_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "datalist = get_data_list_h36m(annot_dir='../H36M/annotations', fitting_thr=25, subject_list=[1, 5, 6, 7, 8, 9, 11], out_dir='../H36M', store_as_pkl=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-envjup",
   "language": "python",
   "name": "ba-envjup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
