{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.smpl_model._smpl import SMPL as SMPL_metro\n",
    "from modules.utils.image_utils import plot_tensor, to_tensor\n",
    "from modules.smpl_model.config_smpl import *\n",
    "from modules.smpl_model.smpl_pose2mesh import SMPL\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMPL Model Pose2Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clara/Desktop/MeineProjekte/HMR_3DWP/modules/smpl_model/smplpytorch/smplpytorch/pytorch/smpl_layer.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459065530/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  torch.Tensor(smpl_data['betas'].r).unsqueeze(0))\n"
     ]
    }
   ],
   "source": [
    "smpl = SMPL()\n",
    "smpl_metro = SMPL_metro()\n",
    "\n",
    "# Adapt Regressor for 10 PCs\n",
    "smpl.layer['neutral'].th_shapedirs= smpl.layer['neutral'].th_shapedirs[:,:,:10]\n",
    "smpl.layer['neutral'].th_betas= smpl.layer['neutral'].th_betas[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocess Data Human 3.6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.data_utils_h36m import get_cam_pose_intr, get_fitting_error\n",
    "from modules.utils.geometry import get_smpl_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datalist of all images with smpl params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject_list = [1, 5, 6, 7, 8, 9, 11]\n",
    "#subject_list = [1, 5, 6, 7, 8]\n",
    "#subject_list = [1]\n",
    "#subject_list = [5]\n",
    "#subject_list = [6]\n",
    "#subject_list = [7]\n",
    "#subject_list = [8]\n",
    "#subject_list = [9]\n",
    "subject_list = [11]\n",
    "data_path = '../H36M'\n",
    "img_dir = osp.join(data_path, 'images')\n",
    "annot_dir = osp.join(data_path, 'annotations')\n",
    "fitting_thr = 25  # milimeter --> Threshhold joints from smpl mesh to h36m gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "cameras = {}\n",
    "smpl_params = {}\n",
    "joints = {}\n",
    "bboxes = {}\n",
    "#length_sub = {sub: 0 for sub in subject_list}\n",
    "for subject in subject_list:\n",
    "    ### Load image annotations\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_data.json'), 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    images.extend(annotations['images'])\n",
    "    ### Load cameras\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_camera.json'), 'r') as f:\n",
    "        cams = json.load(f)\n",
    "    cameras[str(subject)] = {cam_id: get_cam_pose_intr(cam) for cam_id, cam in cams.items()}\n",
    "    ### Load fitted smpl parameter\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_smpl_param.json'), 'r') as f:\n",
    "        smpl_params[str(subject)] = json.load(f)\n",
    "    ### Load 3d Joint ground truth (17x3)\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_joint_3d.json'), 'r') as f:\n",
    "        joints[str(subject)] = json.load(f)   \n",
    "\n",
    "with open(osp.join(annot_dir, 'h36m_bboxes.json'), 'r') as f:\n",
    "        bboxes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_imgs = {} # Maps img-id to img file\n",
    "id_to_imgs = {img['id']: img for img in images}\n",
    "\n",
    "datalist = []\n",
    "num_smpl_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231151/231151 [00:03<00:00, 65405.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_id, img in tqdm(id_to_imgs.items(), total=len(id_to_imgs.items())):\n",
    "    img_name =  img['file_name']  \n",
    "    subject = img['subject']\n",
    "    action = img['action_idx']\n",
    "    subaction = img['subaction_idx']\n",
    "    frame = img['frame_idx']\n",
    "    \n",
    "    ### check if smpl parameters exist\n",
    "    try:\n",
    "        smpl_param = smpl_params[str(subject)][str(action)][str(subaction)][str(frame)]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    ### check threshhold of h36m gt and smpl-mesh h36m joints\n",
    "    joint3d_smpl = np.array(smpl_param['fitted_3d_pose'], np.float32)\n",
    "    joint3d_h36m_gt = np.array(joints[str(subject)][str(action)][str(subaction)][str(frame)],\n",
    "                        dtype=np.float32)\n",
    "    if get_fitting_error(joint3d_h36m_gt, joint3d_smpl) > fitting_thr: \n",
    "        continue\n",
    "        \n",
    "    cam_id = img['cam_idx']\n",
    "    cam_param = cameras[str(subject)][str(cam_id)]\n",
    "    cam_pose, cam_intr = torch.FloatTensor(cam_param['cam_pose']), torch.FloatTensor(cam_param['cam_intr'])\n",
    "    \n",
    "    beta = torch.FloatTensor(smpl_param['shape'])\n",
    "    pose = torch.FloatTensor(smpl_param['pose'])\n",
    "    trans = torch.FloatTensor(smpl_param['trans']) \n",
    "    ### World coordinate --> Camera coordinate\n",
    "    \n",
    "    bbox = bboxes[str(img_id)]\n",
    "    datalist.append({\n",
    "        'img_name': img_name,\n",
    "        'img_id': img_id,\n",
    "        'zarr_id': num_smpl_param,\n",
    "        'betas': beta,\n",
    "        'poses': pose,\n",
    "        'trans': trans,\n",
    "        'bbox': bbox,\n",
    "        'cam_id': cam_id,\n",
    "        'subject': subject,\n",
    "        'cam_pose': cam_pose,\n",
    "        'cam_intr': cam_intr,\n",
    "        })\n",
    "    \n",
    "    num_smpl_param += 1\n",
    "datalist = sorted(datalist, key=lambda x: x['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create joint datapickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject_list = [1, 5, 6, 7, 8, 9, 11]\n",
    "#subject_list = [1, 5, 6, 7, 8]\n",
    "subject_list = [9, 11]\n",
    "fitting_thr = 25  # milimeter --> Threshhold joints from smpl mesh to h36m gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "../H36M/data_pickle/datalist_h36m_thr25_9to9subj.pickle\n",
      "11\n",
      "../H36M/data_pickle/datalist_h36m_thr25_11to11subj.pickle\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "for subj in subject_list:\n",
    "    print(subj)\n",
    "    out_dir = '../H36M/data_pickle'\n",
    "    sub_str = f'{subj}to{subj}'\n",
    "    data_pickle = osp.join(out_dir, f'datalist_h36m_thr{fitting_thr}_{sub_str}subj.pickle')\n",
    "    print(data_pickle)\n",
    "    with open(data_pickle , \"rb\") as fp:\n",
    "        datalist_aux = pkl.load(fp)\n",
    "    datalist = datalist + datalist_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74032"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opt: Store as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../H36M/data_pickle'\n",
    "sub_str = f'{min(subject_list)}to{max(subject_list)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../H36M/data_pickle/datalist_h36m_thr25_9to11subj.pickle'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = osp.join(out_dir, f'datalist_h36m_thr{fitting_thr}_{sub_str}subj.pickle')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'wb') as fp:\n",
    "    pkl.dump(datalist, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c500d23ff985f6e0d55aaada0dd5a9d76ecd6f50f4f40e50dce0e2bcc3272a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ba-envjup': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
