{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-dff2fb292c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmpl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMPL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmpl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_smpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_visualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modules'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from modules.smpl_model._smpl import SMPL\n",
    "from modules.smpl_model.config_smpl import *\n",
    "from modules.utils.image_utils import to_tensor, transform, transform_visualize, crop_box\n",
    "\n",
    "\n",
    "class Human36M(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                data_path:str='../H36M',\n",
    "                split:str = 'train',\n",
    "                num_required_keypoints:int= 0,\n",
    "                debug:bool=True,\n",
    "                store_images=True,\n",
    "                load_from_zarr:str=None,\n",
    "                img_size=224):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.split = split\n",
    "        self.img_dir = osp.join(data_path, 'Human36M', 'images')\n",
    "        self.annot_path = osp.join(data_path, 'Human36M', 'annotations')\n",
    "        self.action_name = ['Directions', 'Discussion', 'Eating', 'Greeting', 'Phoning', 'Posing', 'Purchases',\n",
    "                            'Sitting', 'SittingDown', 'Smoking', 'Photo', 'Waiting', 'Walking', 'WalkDog',\n",
    "                            'WalkTogether']\n",
    "        self.fitting_thr = 25  # milimeter --> Threshhold joints from smpl mesh to h36m gt\n",
    "        self.subject_list = [1, 5, 6, 7, 8, 9, 11]\n",
    "        self.datalist, skip_idx, skip_img_path = self.load_data()\n",
    "        \n",
    "        if self.load_from_zarr is not None:\n",
    "            self.imgs = torch.from_numpy(zarr.load(self.load_from_zarr)) ### Load array into memory\n",
    "        elif self.store_images:\n",
    "            self.img_size = img_size\n",
    "            self.img_cache_indicator = torch.zeros(self.__len__(), dtype=torch.bool)\n",
    "            self.img_cache = torch.empty(self.__len__(), 3, img_size, img_size, dtype=torch.float32)\n",
    "\n",
    "    def load_data(self):\n",
    "        #data_dict = {'images': [], 'annotation': []}\n",
    "        data_dict = defaultdict(list)\n",
    "        cameras = {}\n",
    "        smpl_params = {}\n",
    "        for subject in self.subject_list:\n",
    "            ### Load data and image annotations\n",
    "            with open(osp.join(self.annot_path, 'Human36M_subject' + str(subject) + '_data.json'), 'r') as f:\n",
    "                annotations = json.load(f)\n",
    "            data_dict['images'].append(annotations['images'])\n",
    "            data_dict['annotation'].append(annotations['annotations'])\n",
    "            ### Load cameras\n",
    "            with open(osp.join(self.annot_path, 'Human36M_subject' + str(subject) + '_camera.json'), 'r') as f:\n",
    "                cams = json.load(f)\n",
    "            cameras[str(subject)] = {cam_id: get_cam_pose_intr(cam) for cam_id, cam in cams.items()}\n",
    "            ### Load fitted smpl parameter\n",
    "            with open(osp.join(self.annot_path, 'Human36M_subject' + str(subject) + '_smpl_param.json'), 'r') as f:\n",
    "                smpl_params[str(subject)] = json.load(f)\n",
    "            ### Load 3d Joint ground truth (17x3)\n",
    "            with open(osp.join(self.annot_path, 'Human36M_subject' + str(subject) + '_joint_3d.json'), 'r') as f:\n",
    "                joints[str(subject)] = json.load(f)\n",
    "                \n",
    "        id_to_imgs = {} # Maps ann/img-id to ann file ((image/annotation)'id' = 'image_id')\n",
    "        id_to_anns = {} # Maps ann/img-id to annotation file\n",
    "        \n",
    "        for ann, img in zip(dataset['annotations'], dataset['images']):\n",
    "            id_to_anns[ann['image_id']] = ann \n",
    "            id_to_imgs[img['id']] = img\n",
    "        \n",
    "        datalist = []\n",
    "    \n",
    "        for img_id, img in id_to_imgs.items():\n",
    "            img_path = osp.join(self.img_dir, img['file_name'])  \n",
    "            # check smpl parameter exist\n",
    "            subject = img['subject'];\n",
    "            action = img['action_idx'];\n",
    "            subaction = img['subaction_idx'];\n",
    "            frame = img['frame_idx']; \n",
    "            try:\n",
    "                smpl_param = smpl_params[str(subject)][str(action)][str(subaction)][str(frame)]\n",
    "            except KeyError:\n",
    "                continue \n",
    "                \n",
    "            joint3d_smpl = np.array(smpl_param['fitted_3d_pose'], np.float32)\n",
    "            joint3d_h36m_gt = np.array(joints[str(subject)][str(action_idx)][str(subaction_idx)][str(frame_idx)],\n",
    "                                   dtype=np.float32)\n",
    "            if self.get_fitting_error(joint3d_h36m_gt, joint3d_smpl) > self.fitting_thr: #check threshhold of h36m gt and smpl-mesh h36m joints\n",
    "                continue\n",
    "            beta = torch.FloatTensor(smpl_param['shape'])\n",
    "            pose = torch.FloatTensor(smpl_param['pose'])\n",
    "            trans = torch.FloatTensor(smpl_param['trans'])           \n",
    "            cam_id = img['cam_idx']\n",
    "            cam_param = cameras[str(subject)][str(cam_idx)]\n",
    "            cam_pose, cam_intr = torch.FloatTensor(cam_param['cam_pose']), torch.FloatTensor(cam_param['cam_intr'])\n",
    "            bbox = id_to_anns[img_id]['bbox']\n",
    "            datalist.append({\n",
    "                'img_path': img_path,\n",
    "                'img_id': image_id,\n",
    "                'betas': beta,\n",
    "                'poses': pose,\n",
    "                'trans': trans,\n",
    "                'bbox': bbox,\n",
    "                'cam_pose': cam_param,\n",
    "                'cam_intr': cam_intr,\n",
    "                })\n",
    "\n",
    "        datalist = sorted(datalist, key=lambda x: x['img_id'])\n",
    "        return datalist\n",
    "\n",
    "    def get_fitting_error(self, joint3d_h36m_gt, joint3d_smpl):\n",
    "        joint3d_h36m_gt = joint3d_h36m_gt - joint3d_h36m_gt[H36M_J17_NAME.index('Pelvis'), None,:] # root-relative\n",
    "        # translation alignment\n",
    "        joint3d_smpl = joint3d_smpl - np.mean(joint3d_smpl,0)[None,:] + np.mean(joint3d_h36m_gt,0)[None,:]\n",
    "        error = np.sqrt(np.sum((joint3d_h36m_gt - joint3d_smpl)**2, 1)).mean()\n",
    "        return error\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.datalist[index])\n",
    "        img_id, img_path = data['img_id'], data['img_path']\n",
    "        \n",
    "        if self.load_from_zarr is not None:\n",
    "            img_tensor = self.imgs[index]\n",
    "        elif self.store_images and self.img_cache_indicator[index]:\n",
    "            img_tensor = self.img_cache[index]\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path))\n",
    "            if data['bbox'] is not None:\n",
    "                x_min, y_min, x_max, y_max = data['bbox']\n",
    "                img = img[y_min:y_max, x_min:x_max]\n",
    "            img_tensor = to_tensor(img)\n",
    "            img_tensor = transform(img_tensor, img_size=self.img_size)\n",
    "            if self.store_images:\n",
    "                self.img_cache[index] = img_tensor\n",
    "                self.img_cache_indicator[index] = True\n",
    "        data['img'] = img_tensor\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1:{3:4}, 2: 4, 5:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {3: 4}\n",
      "2 {4: 5}\n"
     ]
    }
   ],
   "source": [
    "for b, v in a.items():\n",
    "    print(b, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_pose_intr(cam_dict):\n",
    "    cam_pose = torch.cat((torch.FloatTensor(cam_dict['R']), torch.FloatTensor(cam_dict['t'])[:,None]), dim = 1)\n",
    "    cam_pose = torch.cat((cam_pose, torch.FloatTensor([[0, 0, 0, 1]])), dim=0)\n",
    "    cam_intr = torch.zeros(3,3)\n",
    "    cam_intr[0,0], cam_intr[1,1] = cam_dict['f']\n",
    "    cam_intr[0:2,2] = torch.tensor(cam_dict['c'])\n",
    "    cam_intr[2,2] = 1.\n",
    "    return {'cam_pose': cam_pose, 'cam_intr': cam_intr}\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-envjup",
   "language": "python",
   "name": "ba-envjup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
