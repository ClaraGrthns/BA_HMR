{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMPL Model Pose2Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import os.path as osp\n",
    "import json\n",
    "\n",
    "from smplpytorch.smplpytorch.pytorch.smpl_layer import SMPL_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMPL(object):\n",
    "    def __init__(self):\n",
    "        self.layer = {'male': self.get_layer('male'), 'female': self.get_layer('female'),\n",
    "                      'neutral': self.get_layer('neutral')}\n",
    "        self.vertex_num = 6890\n",
    "        self.face = self.layer['neutral'].th_faces.numpy()\n",
    "        self.joint_regressor = self.layer['neutral'].th_J_regressor.numpy().astype(np.float32)  # smpl joint regressor\n",
    "        self.joint_regressor_h36m = np.load(osp.join('./data', 'J_regressor_h36m_correct.npy')).astype(np.float32)\n",
    "        #  'J_regressor_coco_use_extra_joint.npy' 'J_regressor_coco_all_smpl_joint.npy'\n",
    "        # add nose, L/R eye, L/R ear\n",
    "        self.face_kps_vertex = (331, 2802, 6262, 3489, 3990)  # mesh vertex idx\n",
    "        nose_onehot = np.array([1 if i == 331 else 0 for i in range(self.joint_regressor.shape[1])],\n",
    "                               dtype=np.float32).reshape(1, -1)\n",
    "        left_eye_onehot = np.array([1 if i == 2802 else 0 for i in range(self.joint_regressor.shape[1])],\n",
    "                                   dtype=np.float32).reshape(1, -1)\n",
    "        right_eye_onehot = np.array([1 if i == 6262 else 0 for i in range(self.joint_regressor.shape[1])],\n",
    "                                    dtype=np.float32).reshape(1, -1)\n",
    "        left_ear_onehot = np.array([1 if i == 3489 else 0 for i in range(self.joint_regressor.shape[1])],\n",
    "                                   dtype=np.float32).reshape(1, -1)\n",
    "        right_ear_onehot = np.array([1 if i == 3990 else 0 for i in range(self.joint_regressor.shape[1])],\n",
    "                                    dtype=np.float32).reshape(1, -1)\n",
    "        self.joint_regressor = np.concatenate(\n",
    "            (self.joint_regressor, nose_onehot, left_eye_onehot, right_eye_onehot, left_ear_onehot, right_ear_onehot))\n",
    "\n",
    "        self.joint_num = 29  # original: 24. manually add nose, L/R eye, L/R ear\n",
    "        self.joints_name = (\n",
    "        'Pelvis', 'L_Hip', 'R_Hip', 'Torso', 'L_Knee', 'R_Knee', 'Spine', 'L_Ankle', 'R_Ankle', 'Chest', 'L_Toe',\n",
    "        'R_Toe', 'Neck', 'L_Thorax', 'R_Thorax', 'Head', 'L_Shoulder', 'R_Shoulder', 'L_Elbow', 'R_Elbow', 'L_Wrist',\n",
    "        'R_Wrist', 'L_Hand', 'R_Hand', 'Nose', 'L_Eye', 'R_Eye', 'L_Ear', 'R_Ear')\n",
    "        self.part_segments_color = ('silver', 'blue', 'green', 'salmon', 'turquoise', 'olive', 'lavender', 'darkblue', 'lime', 'khaki', 'cyan', 'darkgreen',\n",
    "                                    'beige', 'coral', 'crimson', 'red', 'aqua', 'chartreuse', 'indigo', 'teal', 'violet', 'orchid', 'orange', 'gold')\n",
    "        self.flip_pairs = (\n",
    "        (1, 2), (4, 5), (7, 8), (10, 11), (13, 14), (16, 17), (18, 19), (20, 21), (22, 23), (25, 26), (27, 28))\n",
    "        self.skeleton = (\n",
    "        (0, 1), (1, 4), (4, 7), (7, 10), (0, 2), (2, 5), (5, 8), (8, 11), (0, 3), (3, 6), (6, 9), (9, 14), (14, 17),\n",
    "        (17, 19), (21, 23), (9, 13), (13, 16), (16, 18), (18, 20), (20, 22), (9, 12), (12, 24), (24, 14), (24, 25),\n",
    "        (24, 26), (25, 27), (26, 28))\n",
    "        self.root_joint_idx = self.joints_name.index('Pelvis')\n",
    "\n",
    "    def get_layer(self, gender):\n",
    "        gender='neutral'\n",
    "        return SMPL_Layer(gender=gender, model_root='../smpl_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocess Data Human 3.6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle as pkl\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from modules.smpl_model.config_smpl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_pose_intr(cam_dict):\n",
    "    cam_pose = torch.cat((torch.FloatTensor(cam_dict['R']), torch.FloatTensor(cam_dict['t'])[:,None]), dim = 1)\n",
    "    cam_pose = torch.cat((cam_pose, torch.FloatTensor([[0, 0, 0, 1]])), dim=0)\n",
    "    cam_intr = torch.zeros(3,3)\n",
    "    cam_intr[0,0], cam_intr[1,1] = cam_dict['f']\n",
    "    cam_intr[0:2,2] = torch.tensor(cam_dict['c'])\n",
    "    cam_intr[2,2] = 1.\n",
    "    return {'cam_pose': cam_pose, 'cam_intr': cam_intr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitting_error(joint3d_h36m_gt, joint3d_smpl):\n",
    "    joint3d_h36m_gt = joint3d_h36m_gt - joint3d_h36m_gt[H36M_J17_NAME.index('Pelvis'), None,:] # root-relative\n",
    "    # translation alignment\n",
    "    joint3d_smpl = joint3d_smpl - np.mean(joint3d_smpl,0)[None,:] + np.mean(joint3d_h36m_gt,0)[None,:]\n",
    "    error = np.sqrt(np.sum((joint3d_h36m_gt - joint3d_smpl)**2, 1)).mean()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datalist of all images with smpl params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [1, 5, 6, 7, 8, 9, 11]\n",
    "data_path = '../H36M'\n",
    "img_dir = osp.join(data_path, 'images')\n",
    "annot_dir = osp.join(data_path, 'annotations')\n",
    "fitting_thr = 25  # milimeter --> Threshhold joints from smpl mesh to h36m gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "cameras = {}\n",
    "smpl_params = {}\n",
    "joints = {}\n",
    "bboxes = {}\n",
    "for subject in subject_list:\n",
    "    ### Load image annotations\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_data.json'), 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    images.extend(annotations['images'])\n",
    "    ### Load cameras\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_camera.json'), 'r') as f:\n",
    "        cams = json.load(f)\n",
    "    cameras[str(subject)] = {cam_id: get_cam_pose_intr(cam) for cam_id, cam in cams.items()}\n",
    "    ### Load fitted smpl parameter\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_smpl_param.json'), 'r') as f:\n",
    "        smpl_params[str(subject)] = json.load(f)\n",
    "    ### Load 3d Joint ground truth (17x3)\n",
    "    with open(osp.join(annot_dir, 'Human36M_subject' + str(subject) + '_joint_3d.json'), 'r') as f:\n",
    "        joints[str(subject)] = json.load(f)   \n",
    "\n",
    "with open(osp.join(annot_dir, 'h36m_bboxes.json'), 'r') as f:\n",
    "        bboxes = json.load(f)\n",
    "\n",
    "id_to_imgs = {} # Maps img-id to img file\n",
    "id_to_imgs = {img['id']: img for img in images}\n",
    "\n",
    "datalist = []\n",
    "num_smpl_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "for img_id, img in id_to_imgs.items():\n",
    "    img_name =  img['file_name']  \n",
    "    img_id = img['id']\n",
    "\n",
    "    subject = img['subject']\n",
    "    action = img['action_idx']\n",
    "    subaction = img['subaction_idx']\n",
    "    frame = img['frame_idx']; \n",
    "\n",
    "    ### check if smpl parameters exist\n",
    "    try:\n",
    "        smpl_param = smpl_params[str(subject)][str(action)][str(subaction)][str(frame)]\n",
    "    except KeyError:\n",
    "        continue \n",
    "    ### check threshhold of h36m gt and smpl-mesh h36m joints\n",
    "    joint3d_smpl = np.array(smpl_param['fitted_3d_pose'], np.float32)\n",
    "    joint3d_h36m_gt = np.array(joints[str(subject)][str(action)][str(subaction)][str(frame)],\n",
    "                        dtype=np.float32)\n",
    "    if get_fitting_error(joint3d_h36m_gt, joint3d_smpl) > fitting_thr: \n",
    "        continue\n",
    "    cam_id = img['cam_idx']\n",
    "    cam_param = cameras[str(subject)][str(cam_id)]\n",
    "    cam_pose, cam_intr = torch.FloatTensor(cam_param['cam_pose']), torch.FloatTensor(cam_param['cam_intr'])\n",
    "    \n",
    "\n",
    "    beta = torch.FloatTensor(smpl_param['shape'])\n",
    "    pose = torch.FloatTensor(smpl_param['pose'])\n",
    "    trans = torch.FloatTensor(smpl_param['trans']) \n",
    "    ### World coordinate --> Camera coordinate\n",
    "    #beta, pose, trans = get_smpl_coord(smpl_param, cam_pose)\n",
    "    #trans = torch.FloatTensor(trans) \n",
    "    #beta = torch.FloatTensor(beta)\n",
    "    #pose = torch.FloatTensor(pose)\n",
    "    \n",
    "    bbox = bboxes[str(img_id)]\n",
    "    datalist.append({\n",
    "        'img_name': img_name,\n",
    "        'img_id': img_id,\n",
    "        'zarr_id': num_smpl_param,\n",
    "        'betas': beta,\n",
    "        'poses': pose,\n",
    "        'trans': trans,\n",
    "        'bbox': bbox,\n",
    "        'cam_pose': cam_pose,\n",
    "        'cam_intr': cam_intr,\n",
    "        })\n",
    "    \n",
    "datalist = sorted(datalist, key=lambda x: x['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383332"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opt: Store as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../H36M'\n",
    "sub_str = f'{min(subject_list)}to{max(subject_list)}'\n",
    "with open(osp.join(out_dir, f'datalist_h36m_thr{fitting_thr}_{sub_str}subj.pickle'), 'wb') as fp:\n",
    "    pkl.dump(datalist, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.smpl_model._smpl import SMPL as SMPL_metro\n",
    "from modules.utils.render import Renderer\n",
    "from modules.utils.geometry import rotation_matrix_to_angle_axis\n",
    "from modules.utils.image_utils import plot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smpl_coord(pose, beta, trans, root_idx, cam_pose, smpl):\n",
    "        # smpl parameters (pose: 1x72 dimension, shape: 1x10 dimension, trans: 1x3)\n",
    "        pose = pose.view(-1, 3)\n",
    "        beta = beta.view(1, -1)\n",
    "        # translation vector from smpl coordinate to h36m world coordinate\n",
    "        trans = np.array(trans, dtype=np.float32).reshape(3)\n",
    "        # camera rotation and translation\n",
    "        R = np.array(cam_pose[:3, :3], dtype=np.float32).reshape(3, 3)\n",
    "        t = np.array(cam_pose[0:3,3], dtype=np.float32).reshape(3)\n",
    "\n",
    "        # change to mean shape if beta is too far from it\n",
    "        beta[(beta.abs() > 3).any(dim=1)] = 0.\n",
    "\n",
    "        # transform world coordinate to camera coordinate\n",
    "        root_pose = pose[root_idx, :].numpy()\n",
    "        angle = np.linalg.norm(root_pose)\n",
    "        root_pose = transforms3d.axangles.axangle2mat(root_pose / angle, angle)\n",
    "        root_pose = np.dot(R, root_pose)\n",
    "        axis, angle = transforms3d.axangles.mat2axangle(root_pose)\n",
    "        root_pose = axis * angle\n",
    "        pose[root_idx] = torch.from_numpy(root_pose)\n",
    "        pose = pose.view(1, -1)\n",
    "        # get mesh and joint coordinates\n",
    "        smpl_mesh_coord, smpl_joint_coord = smpl(pose, beta)\n",
    "\n",
    "        # incorporate face keypoints\n",
    "        smpl_mesh_coord = smpl_mesh_coord.numpy().astype(np.float32).reshape(-1, 3);\n",
    "        smpl_joint_coord = smpl_joint_coord.numpy().astype(np.float32).reshape(-1, 3)\n",
    "        \n",
    "        # compenstate rotation (translation from origin to root joint was not cancled)\n",
    "        smpl_trans = np.array(trans, dtype=np.float32).reshape(3)  # translation vector from smpl coordinate to h36m world coordinate\n",
    "        smpl_trans = np.dot(R, smpl_trans[:, None]).reshape(1, 3) + t.reshape(1, 3) / 1000\n",
    "        root_joint_coord = smpl_joint_coord[root_idx].reshape(1, 3)\n",
    "        smpl_trans = smpl_trans - root_joint_coord + np.dot(R, root_joint_coord.transpose(1, 0)).transpose(1, 0)\n",
    "        \n",
    "        # translation\n",
    "        smpl_mesh_coord += smpl_trans; smpl_joint_coord += smpl_trans\n",
    "\n",
    "        # meter -> milimeter\n",
    "        smpl_mesh_coord *= 1000; smpl_joint_coord *= 1000;\n",
    "\n",
    "        return smpl_mesh_coord, smpl_joint_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mesh(img, cam_pose, cam_intr, smpl, beta=None, pose=None, trans=None, vertices=None):\n",
    "    cam_intr = cam_intr.detach().numpy()\n",
    "    print(cam_pose.shape, cam_intr.shape)\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.cpu().numpy().transpose(1,2,0)\n",
    "    \n",
    "    ## get smpl faces and vertices ##\n",
    "    faces = smpl.faces.cpu().numpy()\n",
    "    if vertices is None:   \n",
    "        vertices = smpl(pose=pose, beta=beta) + trans\n",
    "    \n",
    "    vertices = vertices[0].detach().numpy()\n",
    "    print(faces.shape,vertices.shape)\n",
    "    ## camera: rotation matrix, t, f and center\n",
    "    cam_rot = rotation_matrix_to_angle_axis(cam_pose[None, :3, :3]).detach().numpy().ravel() \n",
    "    cam_t = cam_pose[0:3,3]\n",
    "    cam_f = np.array([cam_intr[0,0],cam_intr[1,1]])\n",
    "    cam_center = cam_intr[0:2,2]\n",
    "    print(cam_rot, cam_t, cam_f, cam_center)\n",
    "    ## Visualize Mesh \n",
    "    renderer = Renderer(faces=faces)\n",
    "    color= 'pink'\n",
    "    focal_length = 1000\n",
    "    rend_img = renderer.render(vertices,\n",
    "                               cam_t= cam_t,\n",
    "                               cam_rot= cam_rot,\n",
    "                               cam_center= cam_center,\n",
    "                               cam_f = cam_f,\n",
    "                               img= img, \n",
    "                               use_bg = True,\n",
    "                               focal_length = focal_length,\n",
    "                               body_color = color)\n",
    "\n",
    "    #return torch.Tensor\n",
    "    rend_img = rend_img.transpose(2,0,1)\n",
    "    rend_img = torch.from_numpy(rend_img.copy())\n",
    "    return rend_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = SMPL()\n",
    "smpl_metro = SMPL_metro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose = datalist[0]['cam_pose']\n",
    "cam_intr = datalist[0]['cam_intr']\n",
    "beta = datalist[0]['betas']\n",
    "pose= datalist[0]['poses']\n",
    "trans = datalist[0]['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt Regressor for 10 PCs\n",
    "smpl.layer['neutral'].th_shapedirs= smpl.layer['neutral'].th_shapedirs[:,:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6890, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_cam, _= get_smpl_coord(pose=pose, beta=beta,trans=trans, root_idx=0, cam_pose=cam_pose, smpl=smpl.layer['neutral'])\n",
    "verts_cam = torch.FloatTensor(verts_cam)[None]\n",
    "verts_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) (3, 3)\n",
      "(13776, 3) (6890, 3)\n",
      "[ 0.433399   2.181117  -1.7972679] tensor([-346.0508,  546.9808, 5474.4810]) [1145.0494 1143.7811] [512.5415 515.4515]\n",
      "1000 1000\n",
      "6890\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rend_img = visualize_mesh(torch.zeros(3,1000,1000), smpl=smpl_metro, cam_pose=cam_pose, cam_intr=cam_intr, vertices=verts_cam)\n",
    "plot_tensor(rend_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-envjup",
   "language": "python",
   "name": "ba-envjup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
